
\documentclass[a4paper, 11pt, DIV=14]{scrartcl}

\author{Bastian Dittmar}
\date{\today}
\subject{Self Driving Car Nano Degree}
\title{Advanced Lane Finding}

\usepackage{wrapfig, floatrow}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{csquotes}
\usepackage{hyperref}
\hypersetup{
     colorlinks   = true,
     citecolor    = blue,
     linkcolor    = blue
}
\usepackage{amsmath}
\usepackage{listings}

\addto\captionsenglish{\renewcommand{\figurename}{Fig.}}

\usepackage[style=authoryear, citestyle=authoryear-icomp, giveninits=true, autolang=hyphen, hyperref=true, minbibnames=3, dashed=false, doi=false, isbn=false, url=false, sorting=nyt, backend=biber]{biblatex}
\setlength{\bibitemsep}{0.5\baselineskip}
\addbibresource{library.bib}

\pagestyle{plain}
\begin{document}
\maketitle


\section{Introduction}
In this project, a software pipeline is implemented to identify the lane boundaries in a video from a front-facing camera on a car. The code to solve this project is provided in a jupyter notebook.

The goals and steps of this project are the following:
\begin{enumerate}
\item Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.
\item Apply a distortion correction to raw images.
\item Use color transforms, gradients, etc., to create a thresholded binary image.
\item Apply a perspective transform to rectify binary image (birds-eye view).
\item Detect lane pixels and fit to find the lane boundary.
\item Determine the curvature of the lane and vehicle position with respect to center.
\item Warp the detected lane boundaries back onto the original image.
\item Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.
\end{enumerate}


\section{Camera Calibration}
\label{sec:camera_calibration}
The code for this step is contained in code cells 2 and 3 of the IPython notebook located in ``./AdvancedLaneLines.ipynb''

I start by preparing "object points", which will be the (x, y, z) coordinates of the chessboard corners in the world. Here I am assuming the chessboard is fixed on the (x, y) plane at z=0, such that the object points are the same for each calibration image. Thus, objp is just a replicated array of coordinates, and objpoints will be appended with a copy of it every time I successfully detect all chessboard corners in a test image. imgpoints will be appended with the (x, y) pixel position of each of the corners in the image plane with each successful chessboard detection.

I then used the output objpoints and imgpoints to compute the camera calibration and distortion coefficients using the cv2.calibrateCamera() function. I applied this distortion correction to the test image using the cv2.undistort() function. The result is depicted in figure \ref{fig:camera_calibration}.

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/chessboard10_distorted.jpg}
        \caption{Original chessboard image}
        \label{fig:chessboard_original}
    \end{subfigure}\quad
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/chessboard10_undistorted.jpg}
        \caption{Undistorted warped into image plane}
        \label{fig:chessboard_undistorted}
    \end{subfigure} 
    \caption{Example chessboard images used for camera calibration}
    \label{fig:camera_calibration}
\end{figure}

\section{Pipeline (single images)}

\subsection{Distortion Correction}
The first step is to correct the image for distortion. I used the camera matrix and distortion coefficients I obtained from the camera calibration in section \ref{sec:camera_calibration} with the function cv2.undistort(). The result is shown in figure \ref{fig:test02_undistort}.

\begin{figure}
\includegraphics[width=1.0\textwidth]{images/preprocess_test02__undistort.jpg}
\caption{Undistorted test image test02.jpg}
\label{fig:test02_undistort}
\end{figure}

\subsection{Perspective Transformation}
The second step of my pipeline is the perspective transformation of the image into a bird's view. I chose to perform this step before thresholding the image as I found the result to be better. In particular for gradient thresholds which use a kernel of constant size over the image, the distance of an object to the camera matters in a perspective view. Lines that are further away will appear thinner and would require a smaller kernel size than lines in the front for similar results. The bird's view perspective on the other hand transforms the lines to be more or less equally thick along the whole image.

To find a proper transformation 4 points on the host lane lines were chosen and then transformed into a square. The result is depicted in figure \ref{fig:birds_view} and the code in cell ``Perspective Transform''. The transformation from warping this image is used in the pipeline to transform all images into the bird's view. This allows us to measure curvature of the lane later on.

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/straight_lines.jpg}
    \end{subfigure}\quad
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/straight_lines_warped.jpg}
    \end{subfigure} 
    \caption{Bird's view transformation: 4 points on the lane lines were choses that are in shape of a rectangle in world coordinates (assuming the world is flat)}
    \label{fig:birds_view}
\end{figure}

\subsection{Segmentation}

In the next step the image will be segmented using a variation of thresholding algorithms. Result is a binary image with active pixel that we suspect to be lane pixel. The segmentation is done in code cells ``Thresholding Functions'' and ``Image Preprocessing''.

\subsubsection*{Color Threshold}
The first part of the segmentation is a color threshold that tries to identify bright yellow and white objects in the image as those are the lane colors. To account for variations in illumination like shadows I transformed the Image from the BGR color space into the HSV color space. In the HSV color spaces shadows will have a low V-value. The color itself is selected by hue and saturation. Yellow has high saturation in HSV while white has low saturation. The hue for yellow is rather specific around a value of 20 while white is no specific hue. For yellow i used the HSV-thresholds (0-40), (80-255) and (200-255). For white the HSV-thresholds (20-255), (0-80) and (200-255) seemed to work. The thresholds are first applied on the respective channel separately and then the results are combined with logical-and. Thus a pixel has to fall in all threshold ranges to be selected. This is done in function combined\_channel\_treshold() in cell ``Thresholding Functions''. The results of this step can be seen in figure \ref{fig:color_threshold}.

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/preprocess_test02__hsv_yellow.jpg}
    \end{subfigure}\quad
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/preprocess_test02__hsv_white.jpg}
    \end{subfigure} 
    \caption{Color thresholds for yellow lines (left) and white lines (right) on test02.jpg.}
    \label{fig:color_threshold}
\end{figure}

\subsubsection*{Gradient Threshold}
In addition to the color thresholds I chose to apply a gradient threshold using a Sobel operator in x-direction. This will mostly find vertical edges in the image. While the directional gradient sounds promising in theory as well, I found it too noisy to work with in practice. For the gradient in x-direction I chose the HLS color space and applied the gradient to the L- and S-channel which pick up lines quite well regardless of the illumination. For the former I used a threshold of (40-255) and for the latter a threshold of (25-255). The results are shown in figure \ref{fig:gradient_threshold}. To further reduce noise a median filter with kernel size 9.

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/preprocess_test02__glx.jpg}
    \end{subfigure}\quad
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/preprocess_test02__gsx.jpg}
    \end{subfigure} 
    \caption{Gradient thresholds in x-direction i√≥n the L-channe (left) and S-channel (right) on test02.jpg.}
    \label{fig:gradient_threshold}
\end{figure}

\subsubsection*{Combining Thresholds}
The final binary image is a logical-or combination of all 4 thresholding procedures, meaning any pixel that is active in at least one of the threshold images will be picked. The result is shown in figure \ref{fig:threshold_final} where the lane is clearly visible. While test02.jpg is easy it demonstrates the participation of all 4 thresholds quite well.

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/preprocess_test02__combined-OR.jpg}
    \end{subfigure}\quad
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{images/preprocess_test02__warped.jpg}
    \end{subfigure} 
    \caption{The combination of all threshold images (left) of the undistorted image in bird's view (right).}
    \label{fig:threshold_final}
\end{figure}

\subsection{Attempts to reduce overfitting in the model}





\printbibliography
\end{document}
